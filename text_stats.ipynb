{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import tomotopy as tp\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load german model\n",
    "nlp = spacy.load('de_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse files\n",
    "file_list = []\n",
    "for file in os.listdir('data/all_txt/'):\n",
    "    file_list.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [08:32<00:00, 64.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary of all lemmatized texts\n",
    "texts = dict()\n",
    "for file in tqdm(file_list):\n",
    "    with open(f'data/all_txt/{file}', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.replace('¬\\n', '')\n",
    "        doc = nlp(text)\n",
    "        name = file.split('.')[0]\n",
    "        \n",
    "        texts[name] = doc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = []\n",
    "for name, txt in texts.items():\n",
    "    words = [token.text\n",
    "         for token in txt\n",
    "         if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "    nouns = [token.text\n",
    "         for token in txt\n",
    "         if (not token.is_stop and\n",
    "             not token.is_punct and\n",
    "             token.pos_ == \"NOUN\")]\n",
    "    vocab = set(words)\n",
    "    \n",
    "    # most common tokens\n",
    "    word_freq = Counter(words)\n",
    "    common_words = word_freq.most_common(10)\n",
    "\n",
    "    # most common noun tokens\n",
    "    noun_freq = Counter(nouns)\n",
    "    common_nouns = noun_freq.most_common(10)\n",
    "\n",
    "    # vocabulary size\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    entry = {\n",
    "        'name': name,\n",
    "        'common words': common_words,\n",
    "        'common nouns': common_nouns,\n",
    "        'vocabulary size': vocab_size,\n",
    "    }\n",
    "    \n",
    "    entries.append(entry)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_words = []\n",
    "glob_nouns = []\n",
    "\n",
    "for name, txt in texts.items():\n",
    "    for token in txt:\n",
    "        if not token.is_stop and not token.is_punct and not token.is_space:\n",
    "            glob_words.append(token.text)\n",
    "        if (not token.is_stop and not token.is_punct and token.pos_ == \"NOUN\"):\n",
    "            glob_nouns.append(token.text)\n",
    "\n",
    "glob_vocab = set(glob_words)\n",
    "\n",
    "# most common tokens\n",
    "word_freq = Counter(glob_words)\n",
    "glob_common_words = word_freq.most_common(10)\n",
    "\n",
    "# most common noun tokens\n",
    "noun_freq = Counter(glob_nouns)\n",
    "glob_common_nouns = noun_freq.most_common(10)\n",
    "\n",
    "# vocabulary size\n",
    "glob_vocab_size = len(glob_vocab)\n",
    "\n",
    "glob_entry = {\n",
    "    'name': 'all texts',\n",
    "    'common words': glob_common_words,\n",
    "    'common nouns': glob_common_nouns,\n",
    "    'vocabulary size': glob_vocab_size,\n",
    "}\n",
    "\n",
    "entries.append(glob_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'all texts',\n",
       " 'common words': [('$', 922),\n",
       "  ('Töne', 662),\n",
       "  ('c', 605),\n",
       "  ('e', 543),\n",
       "  ('Terz', 476),\n",
       "  ('g', 450),\n",
       "  ('Ton', 436),\n",
       "  ('C', 435),\n",
       "  ('Folge', 407),\n",
       "  ('Bedeutung', 406)],\n",
       " 'common nouns': [('Töne', 662),\n",
       "  ('Terz', 439),\n",
       "  ('Ton', 435),\n",
       "  ('Folge', 407),\n",
       "  ('Bedeutung', 406),\n",
       "  ('Tonart', 352),\n",
       "  ('Grundton', 348),\n",
       "  ('B.', 285),\n",
       "  ('Accorde', 282),\n",
       "  ('Dissonanz', 254)],\n",
       " 'vocabulary size': 19576}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.DataFrame(data=entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('text_stats.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token.text\n",
    "         for token in doc\n",
    "         if not token.is_stop and not token.is_punct and not token.is_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = [token.text\n",
    "         for token in doc\n",
    "         if (not token.is_stop and\n",
    "             not token.is_punct and\n",
    "             token.pos_ == \"NOUN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common tokens\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common(15)\n",
    "\n",
    "# most common noun tokens\n",
    "noun_freq = Counter(nouns)\n",
    "common_nouns = noun_freq.most_common(15)\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', 161),\n",
       " ('S.', 159),\n",
       " ('e', 159),\n",
       " ('g', 128),\n",
       " ('C', 122),\n",
       " ('$', 118),\n",
       " ('Riemann', 104),\n",
       " ('d', 92),\n",
       " ('h', 85),\n",
       " ('f', 82),\n",
       " ('Fig', 71),\n",
       " ('Moll', 62),\n",
       " ('GRAPHIC', 58),\n",
       " ('Terz', 55),\n",
       " ('Grundton', 51)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S.', 154),\n",
       " ('Terz', 52),\n",
       " ('Grundton', 51),\n",
       " ('Töne', 47),\n",
       " ('Auffassung', 36),\n",
       " ('Dur', 35),\n",
       " ('Tonika', 34),\n",
       " ('Umkehrung', 33),\n",
       " ('Konsonanz', 33),\n",
       " ('Akkord', 32),\n",
       " ('g', 30),\n",
       " ('Ton', 29),\n",
       " ('Unterdominante', 29),\n",
       " ('Septime', 28),\n",
       " ('Sexte', 27)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4921"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13368"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = tp.LDAModel(k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt in texts.values():\n",
    "    words = [token.text\n",
    "         for token in txt\n",
    "         if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "    for i in range(0,len(words),100):\n",
    "        mdl.add_doc(words[i: i+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words of topic #0\n",
      "[('c', 0.041490588337183), ('g', 0.030912037938833237), ('e', 0.02754613384604454), ('d', 0.023974156007170677), ('f', 0.021776016801595688), ('h', 0.01476944237947464), ('1', 0.013739064335823059), ('phon', 0.009686242789030075), ('=', 0.008930631913244724), ('Klänge', 0.008449789136648178)]\n",
      "Top 10 words of topic #1\n",
      "[('$', 0.092230886220932), ('MATH', 0.032011374831199646), ('Töne', 0.02130790427327156), ('=', 0.01390550471842289), ('Terz', 0.011804825626313686), ('GRAPHIC', 0.011204631067812443), ('Octave', 0.009304014965891838), ('reinen', 0.008903885260224342), ('Intervalle', 0.008803852833807468), ('Schwingungszahlen', 0.008503755554556847)]\n",
      "Top 10 words of topic #2\n",
      "[('Metrum', 0.015257209539413452), ('Einheit', 0.015024293214082718), ('Bestimmung', 0.014209085144102573), ('metrischen', 0.013859710656106472), ('Glied', 0.013626793399453163), ('Ordnung', 0.012811585329473019), ('metrische', 0.011763460002839565), ('Bedeutung', 0.009783667512238026), ('Formation', 0.009550751186907291), ('metrisch', 0.009084917604923248)]\n",
      "Top 10 words of topic #3\n",
      "[('Quint', 0.02680678851902485), ('Terz', 0.022279221564531326), ('Folge', 0.01717672497034073), ('tonischen', 0.012289827689528465), ('Tonart', 0.009846379049122334), ('Dreiklänge', 0.008912119083106518), ('Bedeutung', 0.008912119083106518), ('C-e-G', 0.008696520701050758), ('Verbindung', 0.008552788756787777), ('Grundton', 0.008409056812524796)]\n",
      "Top 10 words of topic #4\n",
      "[('Tonart', 0.021022198721766472), ('Dreiklang', 0.012222617864608765), ('Accorde', 0.011929298751056194), ('B.', 0.011538206599652767), ('erscheinen', 0.007725054398179054), ('Beispiele', 0.007725054398179054), ('Dreiklänge', 0.007431735284626484), ('unten', 0.006845096591860056), ('übrigen', 0.006551777012646198), ('Quinte', 0.006551777012646198)]\n",
      "Top 10 words of topic #5\n",
      "[('S.', 0.023500662297010422), ('Riemann', 0.011319685727357864), ('C', 0.010996297001838684), ('Dur', 0.007439020089805126), ('Fig', 0.00700783496722579), ('Konsonanz', 0.006792242173105478), ('R.', 0.005606483202427626), ('Helmholtz', 0.00549868680536747), ('sogar', 0.005067502148449421), ('Grundton', 0.004851909354329109)]\n",
      "Top 10 words of topic #6\n",
      "[('Verfasser', 0.010565332137048244), ('Fig', 0.00946903321892023), ('2c', 0.008771387860178947), ('Seite', 0.008273070678114891), ('Theorie', 0.007874416187405586), ('alten', 0.006678454112261534), ('Harmoniesystem', 0.0060804723761975765), ('Weitzmann', 0.005881145130842924), ('Harmonielehre', 0.005283163860440254), ('Harmonieen', 0.005183500237762928)]\n",
      "Top 10 words of topic #7\n",
      "[('3', 0.01938798651099205), ('Töne', 0.017362477257847786), ('2', 0.014951156452298164), ('Musik', 0.013407910242676735), ('musikalische', 0.010224967263638973), ('1', 0.009646249935030937), ('4', 0.008874626830220222), ('Art', 0.008681721054017544), ('nämlich', 0.007910098880529404), ('5', 0.007717193104326725)]\n",
      "Top 10 words of topic #8\n",
      "[('Tonarten', 0.01273193210363388), ('Hauptmann', 0.011095087043941021), ('harmonischen', 0.010367599315941334), ('Weise', 0.010003856383264065), ('Gegensatz', 0.008185138925909996), ('Tönen', 0.007821395061910152), ('B', 0.007184843998402357), ('verschiedenen', 0.0070939077995717525), ('Reihe', 0.006912036333233118), ('lassen', 0.006457356736063957)]\n",
      "Top 10 words of topic #9\n",
      "[('C', 0.021779518574476242), ('Töne', 0.01871795952320099), ('Dissonanz', 0.017813406884670258), ('Ton', 0.017604663968086243), ('G', 0.01704801619052887), ('Auflösung', 0.0158651415258646), ('D', 0.015447655692696571), ('Septime', 0.01496058888733387), ('F', 0.014264780096709728), ('h', 0.012873162515461445)]\n"
     ]
    }
   ],
   "source": [
    "for k in range(mdl.k):\n",
    "    print('Top 10 words of topic #{}'.format(k))\n",
    "    print(mdl.get_topic_words(k, top_n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-e304f8207c9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtopic_term_dists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_topic_word_dist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdoc_topic_dists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_topic_dist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdoc_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "\n",
    "topic_term_dists = np.stack([mdl.get_topic_word_dist(k) for k in range(mdl.k)])\n",
    "doc_topic_dists = np.stack([doc.get_topic_dist() for doc in mdl.docs])\n",
    "doc_lengths = np.array([len(doc.words) for doc in mdl.docs])\n",
    "vocab = list(mdl.used_vocabs)\n",
    "term_frequency = mdl.used_vocab_freq\n",
    "\n",
    "prepared_data = pyLDAvis.prepare(\n",
    "    topic_term_dists,\n",
    "    doc_topic_dists,\n",
    "    doc_lengths,\n",
    "    vocab,\n",
    "    term_frequency,\n",
    "    sort_topics=False,\n",
    "    start_index=0,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(prepared_data, \"ldavis.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(prepared_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
